{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import skimage\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "plt.ion()   # interactive mode\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    \"\"\"\n",
    "    Load an image into PIL format and convert it into RGB    \n",
    "    :param path: String, Complete path of the image file\n",
    "    :return: PIL image\n",
    "    \"\"\"\n",
    "    image = Image.open(path)\n",
    "    return image.convert(\"RGB\")\n",
    "    \n",
    "def show_tensor_image(tensor):\n",
    "    \"\"\"\n",
    "    Take a tensor and show the corresponding image\n",
    "    :param tensor: Pytorch Tensor, [channels, height, width]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tensor = tensor.transpose(0, 1)\n",
    "    tensor = tensor.transpose(1, 2)\n",
    "    io.imshow(tensor.cpu().numpy())\n",
    "\n",
    "def create_valid_train_set(csv_info_name, data_clean_dir, dataset_name, test_size):\n",
    "    csv_info_name = os.path.join(data_clean_dir, dataset_name, csv_info_name)\n",
    "    csv_info = pd.read_csv(csv_info_name)\n",
    "    X_train, X_valid = train_test_split(csv_info, test_size=test_size,\n",
    "                                        random_state=42, stratify=csv_info[\"tag\"])\n",
    "    X_train_tmp = X_train[X_train[\"tag\"] != \"idle\"]\n",
    "    \n",
    "    X_train = pd.concat([X_train, *([X_train_tmp] * 4)], axis=0)\n",
    "    return X_train, X_valid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteImageDataset(Dataset):\n",
    "    \"\"\"Load a satellite dataset\"\"\"\n",
    "\n",
    "    def __init__(self, X, transform=transforms.ToTensor(), device=torch.device(\"cpu\")):\n",
    "        \"\"\"\n",
    "        Create a satellite image dataset\n",
    "        :param transform: torchvion transform function, Optional transform to be applied\n",
    "                on an image.\n",
    "        :device: Pytorch device: cpu or gpu to move the data into the good device\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.L_image_path = list(self.X[\"image_clean_path\"])\n",
    "        lab_enc = LabelEncoder()\n",
    "        self.X[\"tag\"] = lab_enc.fit_transform(self.X[\"tag\"])\n",
    "        self.L_tag = list(self.X[\"tag\"])\n",
    "        self.classes_ = lab_enc.classes_\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.L_image_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image = pil_loader(self.L_image_path[idx])\n",
    "        image = self.transform(image)\n",
    "        return image.to(self.device), torch.tensor(self.L_tag[idx]).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size=3, output_size=3):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(input_size, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size=3, output_size=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 29 * 29, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, output_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 29 * 29)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate, train_dataset, valid_dataset, num_workers, path_model, net_name):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                          num_workers=num_workers, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                          num_workers=num_workers, shuffle=True)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Loss function\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    best_valid_loss = np.inf\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            if i == 10:\n",
    "                break\n",
    "            #Get inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.detach().item()\n",
    "            total_train_loss += loss_size.detach().item()\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in valid_loader:\n",
    "                        \n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.detach().item()\n",
    "        \n",
    "        if total_val_loss < best_valid_loss:\n",
    "            best_valid_loss = total_val_loss\n",
    "            torch.save(net.state_dict(), os.path.join(path_model, net_name))\n",
    "\n",
    "            \n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(valid_loader)))\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/classification_speed_boat/data/\"\n",
    "data_clean_dir = \"/classification_speed_boat/data_clean/\"\n",
    "dataset_name = \"train\"\n",
    "csv_info_name = \"info_boat.csv\"\n",
    "csv_clean_name = \"info_boat.csv\"\n",
    "grid_box = [100, 150, 200]\n",
    "\n",
    "model_name = \"cnn\"\n",
    "net_name = \"cnn_basic.pt\"\n",
    "prediction_dir = \"/classification_speed_boat/prediction/\"\n",
    "path_model = os.path.join(prediction_dir, model_name)\n",
    "if not os.path.exists(path_model):\n",
    "    os.makedirs(path_model)\n",
    "csv_preds_name = \"preds.csv\"\n",
    "\n",
    "# Parameters\n",
    "test_size = 0.2\n",
    "\n",
    "size = 128\n",
    "transform = transforms.Compose([transforms.Resize((size, size)),\n",
    "                                transforms.RandomRotation((0, 180)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.RandomErasing()])\n",
    "transform_valid = transforms.Compose([transforms.Resize((size, size)),\n",
    "                                          transforms.ToTensor()])\n",
    "    \n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train, X_valid = create_valid_train_set(csv_clean_name, data_clean_dir, dataset_name, test_size)\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = SatelliteImageDataset(X_train, transform, device)\n",
    "valid_dataset = SatelliteImageDataset(X_valid, transform_valid, device)\n",
    "\n",
    "input_size = 3\n",
    "output_size = 3\n",
    "\n",
    "n_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Net(input_size, output_size)\n",
    "cnn = cnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNet(cnn, batch_size, n_epochs, learning_rate, train_dataset, valid_dataset, num_workers, path_model, net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Net(input_size, output_size)\n",
    "\n",
    "cnn.load_state_dict(torch.load(os.path.join(path_model, net_name)))\n",
    "\n",
    "cnn = cnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                          num_workers=num_workers, shuffle=True)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                          num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, X):\n",
    "    \"\"\"\n",
    "    return the prediction of the read2genome algorithm\n",
    "    :param X: Tensor [batch_size, n_dim], matrix\n",
    "    :return: 1-D torch Tensor\n",
    "    \"\"\"\n",
    "    y = torch.nn.Softmax(dim=1)(net.eval()(X))\n",
    "    return torch.argmax(y, dim=1), y\n",
    "\n",
    "def compute_score(net, dataloader):\n",
    "    \"\"\"Return the classification score rate for a dataset\n",
    "    in a dataloader\"\"\"\n",
    "    with torch.no_grad():\n",
    "        nb_elem = 0\n",
    "        score = 0\n",
    "        L_y_true = []\n",
    "        L_y_pred = []\n",
    "        L_y_softmax = []\n",
    "        for it, (X, y_) in enumerate(iter(dataloader)):\n",
    "            y, y_softmax = predict(net, X)\n",
    "            L_y_true += list(y_.cpu().numpy())\n",
    "            L_y_pred += list(y.cpu().numpy())\n",
    "            L_y_softmax += list(y_softmax.cpu().numpy())\n",
    "            nb_elem += len(X)\n",
    "            score += (y.int() == y_.int()).sum().item()\n",
    "        return score * 1. / nb_elem, np.array(L_y_true), np.array(L_y_pred), np.array(L_y_softmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, y_true, y_pred, y_softmax = compute_score(cnn, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(list(zip(list(valid_dataset.X[\"record_id\"]), list(y_softmax))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(y_true, y_pred):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    seaborn.heatmap(confusion_matrix(y_true, y_pred), cmap=\"YlGnBu\")\n",
    "    plt.xlabel(\"Predictions\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.savefig(os.path.join(path_model, \"confusion_matrix.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.utils.multiclass import unique_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = plot_confusion_matrix(y_true, y_pred, train_dataset.classes_,\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test.get_figure().savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
